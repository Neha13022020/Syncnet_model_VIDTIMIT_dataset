{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BxgujWK9RZac",
    "outputId": "9f5d1d0b-a3e4-4be9-989b-de61a3523c80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "# mounting drive\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "RNq6OtVKSF8C"
   },
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import h5py\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, BatchNormalization, Activation, MaxPooling2D\n",
    "from keras.layers import Flatten, Dense\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import sys\n",
    "import dlib\n",
    "import moviepy.editor as mpy\n",
    "import wave\n",
    "import scipy.io.wavfile as wav\n",
    "import contextlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VEUDHWgdRsvx",
    "outputId": "871a97ea-034e-4fd7-ca79-c67e2e7e6853"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting speechpy\n",
      "  Downloading https://files.pythonhosted.org/packages/8f/12/dbda397a998063d9541d9e149c4f523ed138a48824d20598e37632ba33b1/speechpy-2.4-py2.py3-none-any.whl\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from speechpy) (1.18.5)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from speechpy) (1.4.1)\n",
      "Installing collected packages: speechpy\n",
      "Successfully installed speechpy-2.4\n"
     ]
    }
   ],
   "source": [
    "pip install speechpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "l-QJU-6zUjym"
   },
   "outputs": [],
   "source": [
    "import speechpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "j5YejjgnRssj"
   },
   "outputs": [],
   "source": [
    "#http://www.practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/\n",
    "\n",
    "def audio_processing(wav_file, verbose):\n",
    "    ''' takes audio file as input and creates mfcc features '''\n",
    "    \n",
    "    \"\"\"To extract mfcc features of audio, clips 0.2 seconds in length each,\n",
    "    i.e. of 20 MFCC features in each clip (acc. to syncnet paper)\n",
    "    Output mfcc_clips shape === (N, 12, 20, 1),\n",
    "    where N = len(mfcc_features) // 20\n",
    "    \"\"\"\n",
    "\n",
    "    rate, sig = wav.read(wav_file)\n",
    "    if verbose:\n",
    "        print(\"Sig length: {}, sample_rate: {}\".format(len(sig), rate))\n",
    "\n",
    "    try:\n",
    "        mfcc_features = speechpy.feature.mfcc(sig, sampling_frequency=rate, frame_length=0.010, frame_stride=0.010)\n",
    "    except IndexError:\n",
    "        raise ValueError(\"ERROR: Index error occurred while extracting mfcc\")\n",
    "\n",
    "    if verbose:\n",
    "        print(\"mfcc_features shape:\", mfcc_features.shape)\n",
    "\n",
    "    # Number of audio clips = len(mfcc_features) // length of each audio clip\n",
    "    number_of_audio_clips = len(mfcc_features) // AUDIO_TIME_STEPS\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Number of audio clips:\", number_of_audio_clips)\n",
    "\n",
    "    # Don't consider the first MFCC feature, only consider the next 12 (Checked in syncnet_demo.m)\n",
    "    # Also, only consider AUDIO_TIME_STEPS*number_of_audio_clips features\n",
    "    mfcc_features = mfcc_features[:AUDIO_TIME_STEPS*number_of_audio_clips, 1:]\n",
    "\n",
    "    # Reshape mfcc_features from (x, 12) to (x//20, 12, 20, 1)\n",
    "    mfcc_features = np.expand_dims(np.transpose(np.split(mfcc_features, number_of_audio_clips), (0, 2, 1)), axis=-1)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Final mfcc_features shape:\", mfcc_features.shape)\n",
    "    return mfcc_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ywfgV1h0RsqU"
   },
   "outputs": [],
   "source": [
    "def make_rect_shape_square(rect):\n",
    "    # Rect: (x, y, x+w, y+h)\n",
    "\n",
    "    x = rect[0]\n",
    "    y = rect[1]\n",
    "    w = rect[2] - x\n",
    "    h = rect[3] - y\n",
    "    # If width > height\n",
    "    if w > h:\n",
    "        new_x = x\n",
    "        new_y = int(y - (w-h)/2)\n",
    "        new_w = w\n",
    "        new_h = w\n",
    "    # Else (height > width)\n",
    "    else:\n",
    "        new_x = int(x - (h-w)/2)\n",
    "        new_y = y\n",
    "        new_w = h\n",
    "        new_h = h\n",
    "    return [new_x, new_y, new_x + new_w, new_y + new_h]\n",
    "\n",
    "\n",
    "def expand_rect(rect, scale, frame_shape, scale_w=1.5, scale_h=1.5):\n",
    "\n",
    "    if scale is not None:\n",
    "        scale_w = scale\n",
    "        scale_h = scale\n",
    "    # Rect: (x, y, x+w, y+h)\n",
    "    x = rect[0]\n",
    "    y = rect[1]\n",
    "    w = rect[2] - x\n",
    "    h = rect[3] - y\n",
    "    # new_w, new_h\n",
    "    new_w = int(w * scale_w)\n",
    "    new_h = int(h * scale_h)\n",
    "    # new_x\n",
    "    new_x = int(x - (new_w - w)/2)\n",
    "    if new_x < 0:\n",
    "        new_w = new_x + new_w\n",
    "        new_x = 0\n",
    "    elif new_x + new_w > (frame_shape[1] - 1):\n",
    "        new_w = (frame_shape[1] - 1) - new_x\n",
    "    # new_y\n",
    "    new_y = int(y - (new_h - h)/2)\n",
    "    if new_y < 0:\n",
    "        new_h = new_y + new_h\n",
    "        new_y = 0\n",
    "    elif new_y + new_h > (frame_shape[0] - 1):\n",
    "        new_h = (frame_shape[0] - 1) - new_y\n",
    "    return [new_x, new_y, new_x + new_w, new_y + new_h]\n",
    "\n",
    "def detect_mouth_in_frame(frame, detector, predictor, prevFace, verbose):\n",
    "    ''' takes frames as input and detect face and mouth from it, then return it with proper coordinates '''\n",
    "\n",
    "    # Detect all faces\n",
    "    faces = detector(frame, 1)\n",
    "\n",
    "    # If no faces are detected\n",
    "    if len(faces) == 0:\n",
    "        if verbose:\n",
    "            print(\"No faces detected, using prevFace\", prevFace, \"(detect_mouth_in_frame)\")\n",
    "        faces = [prevFace]\n",
    "\n",
    "    # Note first face (ASSUMING FIRST FACE IS THE REQUIRED ONE!)\n",
    "    face = faces[0]\n",
    "    # Predict facial landmarks\n",
    "    shape = predictor(frame, face)\n",
    "    # Note all mouth landmark coordinates\n",
    "    mouthCoords = np.array([[shape.part(i).x, shape.part(i).y] for i in range(48, 68)])\n",
    "\n",
    "    # Mouth Rect: x, y, x+w, y+h\n",
    "    mouthRect = [np.min(mouthCoords[:, 1]), np.min(mouthCoords[:, 0]),\n",
    "                 np.max(mouthCoords[:, 1]), np.max(mouthCoords[:, 0])]\n",
    "\n",
    "    # Make mouthRect square\n",
    "    mouthRect = make_rect_shape_square(mouthRect)\n",
    "\n",
    "    # Expand mouthRect square\n",
    "    expandedMouthRect = expand_rect(mouthRect, scale=(MOUTH_TO_FACE_RATIO * face.width() / mouthRect[2]), frame_shape=(frame.shape[0], frame.shape[1]))\n",
    "    \n",
    "    # Mouth\n",
    "    mouth = frame[expandedMouthRect[1]:expandedMouthRect[3],\n",
    "                  expandedMouthRect[0]:expandedMouthRect[2]]\n",
    "\n",
    "    # # Resize to 120x120\n",
    "    # resizedMouthImage = np.round(resize(mouth, (120, 120), preserve_range=True)).astype('uint8')\n",
    "\n",
    "    # Return mouth\n",
    "    return mouth, face\n",
    "\n",
    "def video_processing(video):\n",
    "  ''' takes video as input and returns array for the detected mouth '''\n",
    "\n",
    "  predictor_path = '/content/gdrive/My Drive/shape_predictor_68_face_landmarks.dat'\n",
    "  detector = dlib.get_frontal_face_detector()\n",
    "  predictor = dlib.shape_predictor(predictor_path)\n",
    "\n",
    "  cap = cv2.VideoCapture(video)\n",
    "  \n",
    "  # Default face rect\n",
    "  face = dlib.rectangle(30, 30, 220, 220)\n",
    "  lip_model_input = []\n",
    "  frame_index = 0\n",
    "  while(cap.isOpened()):\n",
    "          \n",
    "          frames = []\n",
    "          for i in range(5):\n",
    "              _, frame = cap.read()\n",
    "              frame_index += 1\n",
    "              # print(\"Frame\", frame_index+1, \"of\", frameCount, end=\"\\r\")\n",
    "\n",
    "              # If no frame is read, break\n",
    "              if frame is None:\n",
    "                  break\n",
    "              \n",
    "              # Detect mouth in the frame\n",
    "              mouth, _ = detect_mouth_in_frame(frame, detector, predictor, prevFace=face, verbose=False)\n",
    "\n",
    "              # Convert mouth to grayscale\n",
    "              mouth = cv2.cvtColor(mouth, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "              # Resize mouth to syncnet input shape\n",
    "              mouth = cv2.resize(mouth, (MOUTH_W, MOUTH_H))\n",
    "\n",
    "              # Subtract 110 from all mouth values (Checked in syncnet_demo.m)\n",
    "              mouth = mouth - 110.\n",
    "\n",
    "              frames.append(mouth)\n",
    "\n",
    "          if len(frames) == 5:\n",
    "              stacked = np.stack(frames, axis=-1) #syncnet requires (112,112,5)\n",
    "              lip_model_input.append(stacked)\n",
    "          else:\n",
    "              break\n",
    "\n",
    "  return np.array(lip_model_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "_ItE4TxRSWV2"
   },
   "outputs": [],
   "source": [
    "def syncnet_lip_model_v4():\n",
    "    ''' model layers for lip area from video ''' \n",
    "\n",
    "    # Image data format\n",
    "    K.set_image_data_format(IMAGE_DATA_FORMAT)\n",
    "    input_shape = ( MOUTH_H, MOUTH_W, SYNCNET_VIDEO_CHANNELS)\n",
    "\n",
    "    lip_model = Sequential()     # ( None, 112, 112, 5)\n",
    "\n",
    "    # conv1_lip\n",
    "    lip_model.add(Conv2D(96, (3, 3), padding='valid', input_shape=input_shape, name='conv1_lip'))  # (None, 110, 110, 96)\n",
    "    # bn1_lip\n",
    "    lip_model.add(BatchNormalization(name='bn1_lip'))\n",
    "    # relu1_lip\n",
    "    lip_model.add(Activation('relu', name='relu1_lip'))\n",
    "    # pool1_lip\n",
    "    lip_model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='valid', name='pool1_lip'))   # (None, 54, 54, 96)\n",
    "\n",
    "\n",
    "    # conv2_lip\n",
    "    lip_model.add(Conv2D(256, (5, 5), padding='valid', name='conv2_lip'))   # (None, 256, 50, 50)\n",
    "    # bn2_lip\n",
    "    lip_model.add(BatchNormalization(name='bn2_lip'))\n",
    "    # relu2_lip\n",
    "    lip_model.add(Activation('relu', name='relu2_lip'))\n",
    "    # pool2_lip\n",
    "    lip_model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='valid', name='pool2_lip'))   # (None, 24, 24, 256)\n",
    "\n",
    "\n",
    "    # conv3_lip\n",
    "    lip_model.add(Conv2D(512, (3, 3), padding='valid', name='conv3_lip'))   # (None, 22, 22, 512)\n",
    "    # bn3_lip\n",
    "    lip_model.add(BatchNormalization(name='bn3_lip'))\n",
    "    # relu3_lip\n",
    "    lip_model.add(Activation('relu', name='relu3_lip'))\n",
    "\n",
    "\n",
    "    # conv4_lip\n",
    "    lip_model.add(Conv2D(512, (3, 3), padding='valid', name='conv4_lip'))   # (None, 20, 20, 512)\n",
    "    # bn4_lip\n",
    "    lip_model.add(BatchNormalization(name='bn4_lip'))\n",
    "    # relu4_lip\n",
    "    lip_model.add(Activation('relu', name='relu4_lip'))\n",
    "\n",
    "\n",
    "    # conv5_lip\n",
    "    lip_model.add(Conv2D(512, (3, 3), padding='valid', name='conv5_lip'))   # (None, 18, 18, 512)\n",
    "    # bn5_lip\n",
    "    lip_model.add(BatchNormalization(name='bn5_lip'))\n",
    "    # relu5_lip\n",
    "    lip_model.add(Activation('relu', name='relu5_lip'))\n",
    "    # pool5_lip\n",
    "    lip_model.add(MaxPooling2D(pool_size=(3, 3), strides=(3, 3), padding='valid', name='pool5_lip'))   # (None, 6, 6, 512)\n",
    "\n",
    "\n",
    "    # fc6_lip\n",
    "    lip_model.add(Flatten(name='flatten_lip'))\n",
    "    lip_model.add(Dense(256, name='fc6_lip'))    # (None, 256)\n",
    "    # bn6_lip\n",
    "    lip_model.add(BatchNormalization(name='bn6_lip'))\n",
    "    # relu6_lip\n",
    "    lip_model.add(Activation('relu', name='relu6_lip'))\n",
    "\n",
    "\n",
    "    # fc7_lip\n",
    "    lip_model.add(Dense(128, name='fc7_lip'))    # (None, 128)\n",
    "    # bn7_lip\n",
    "    lip_model.add(BatchNormalization(name='bn7_lip'))\n",
    "    # relu7_lip\n",
    "    lip_model.add(Activation('relu', name='relu7_lip'))\n",
    "\n",
    "\n",
    "    return lip_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "W3Gs2_TpSWTn"
   },
   "outputs": [],
   "source": [
    "def syncnet_audio_model_v4():\n",
    "    ''' model layers for audio features '''\n",
    "\n",
    "    # Audio input shape\n",
    "    input_shape = ( SYNCNET_MFCC_CHANNELS, AUDIO_TIME_STEPS, 1)\n",
    "\n",
    "    audio_model = Sequential()     # (None, 12, 20, 1)\n",
    "\n",
    "    # conv1_audio\n",
    "    audio_model.add(Conv2D(64, (3, 3), padding='same', name='conv1_audio', input_shape=input_shape))  # (None, 12, 20, 64)\n",
    "    # bn1_audio\n",
    "    audio_model.add(BatchNormalization(name='bn1_audio'))\n",
    "    # relu1_audio\n",
    "    audio_model.add(Activation('relu', name='relu1_audio'))\n",
    "\n",
    "\n",
    "    # conv2_audio\n",
    "    audio_model.add(Conv2D(128, (3, 3), padding='same', name='conv2_audio'))   # (None, 12, 20, 128)\n",
    "    # bn2_audio\n",
    "    audio_model.add(BatchNormalization(name='bn2_audio'))\n",
    "    # relu2_audio\n",
    "    audio_model.add(Activation('relu', name='relu2_audio'))\n",
    "    # pool2_audio\n",
    "    audio_model.add(MaxPooling2D(pool_size=(1, 3), strides=(1, 2), padding='valid', name='pool2_audio'))   # (None, 12, 9, 128)\n",
    "\n",
    "\n",
    "    # conv3_audio\n",
    "    audio_model.add(Conv2D(256, (3, 3), padding='same', name='conv3_audio'))   # (None, 12, 9, 256)\n",
    "    # bn3_audio\n",
    "    audio_model.add(BatchNormalization(name='bn3_audio'))\n",
    "    # relu3_audio\n",
    "    audio_model.add(Activation('relu', name='relu3_audio'))\n",
    "\n",
    "\n",
    "    # conv4_audio\n",
    "    audio_model.add(Conv2D(256, (3, 3), padding='same', name='conv4_audio'))   # (None, 12, 9, 256)\n",
    "    # bn4_audio\n",
    "    audio_model.add(BatchNormalization(name='bn4_audio'))\n",
    "    # relu4_audio\n",
    "    audio_model.add(Activation('relu', name='relu4_audio'))\n",
    "\n",
    "\n",
    "    # conv5_audio\n",
    "    audio_model.add(Conv2D(256, (3, 3), padding='same', name='conv5_audio'))   # (None, 12, 9, 256)\n",
    "    # bn5_audio\n",
    "    audio_model.add(BatchNormalization(name='bn5_audio'))\n",
    "    # relu5_audio\n",
    "    audio_model.add(Activation('relu', name='relu5_audio'))\n",
    "    # pool5_audio\n",
    "    audio_model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='valid', name='pool5_audio'))   # (None, 5, 4, 256)\n",
    "\n",
    "\n",
    "    # fc6_audio\n",
    "    audio_model.add(Flatten(name='flatten_audio'))\n",
    "    audio_model.add(Dense(256, name='fc6_audio'))    # (None, 256)\n",
    "    # bn6_audio\n",
    "    audio_model.add(BatchNormalization(name='bn6_audio'))\n",
    "    # relu6_audio\n",
    "    audio_model.add(Activation('relu', name='relu6_audio'))\n",
    "\n",
    "\n",
    "    # fc7_audio\n",
    "    audio_model.add(Dense(128, name='fc7_audio'))    # (None, 128)\n",
    "    # bn7_audio\n",
    "    audio_model.add(BatchNormalization(name='bn7_audio'))\n",
    "    # relu7_audio\n",
    "    audio_model.add(Activation('relu', name='relu7_audio'))\n",
    "\n",
    "\n",
    "    return audio_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "X_m5Y4TaSZsw"
   },
   "outputs": [],
   "source": [
    "def load_syncnet_model(mode, verbose):\n",
    "    ''' loading the syncnet model '''\n",
    "    \n",
    "    if mode == 'lip' or mode == 'both':\n",
    "      # Load frontal model\n",
    "      syncnet_lip_model = syncnet_lip_model_v4()\n",
    "\n",
    "    if mode == 'audio' or mode == 'both':   \n",
    "      # Load frontal model\n",
    "      syncnet_audio_model = syncnet_audio_model_v4()\n",
    "       \n",
    "    if mode == 'lip':\n",
    "        syncnet_model = syncnet_lip_model\n",
    "    elif mode == 'audio':\n",
    "        syncnet_model = syncnet_audio_model\n",
    "    elif mode == 'both':\n",
    "        syncnet_model = [syncnet_audio_model, syncnet_lip_model]\n",
    "\n",
    "    return syncnet_model\n",
    "\n",
    "# https://github.com/voletiv/syncnet-in-keras/blob/master/syncnet-weights/syncnet-weights-readme.md\n",
    "\n",
    "def load_syncnet_weights( verbose):\n",
    "    ''' reading and loading pre trained weights file '''\n",
    "\n",
    "    syncnet_weights_file = '/content/gdrive/My Drive/lipsync_v4_73.mat'\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Loading syncnet_weights from\", syncnet_weights_file)\n",
    "\n",
    "    if not os.path.isfile(syncnet_weights_file):\n",
    "        raise ValueError(\n",
    "            \"\\n\\nERROR: syncnet_weight_file missing!! File: \" + syncnet_weights_file + \\\n",
    "            \"\\nPlease specify correct file name in the syncnet_params.py file and relaunch.\\n\")\n",
    "\n",
    "    # Read weights file, with layer names\n",
    "    with h5py.File(syncnet_weights_file, 'r') as f:\n",
    "        syncnet_weights = [f[v[0]][:] for v in f['net/params/value']]\n",
    "        syncnet_layer_names = [[chr(i) for i in  f[n[0]]] \\\n",
    "                               for n in f['net/layers/name']]\n",
    "\n",
    "    # Find the starting index of audio and lip layers\n",
    "    audio_found = False\n",
    "    audio_start_idx = 0\n",
    "    lip_found = False\n",
    "    lip_start_idx = 0\n",
    "\n",
    "    # Join the chars of layer names to make them words\n",
    "    for i in range(len(syncnet_layer_names)):\n",
    "        syncnet_layer_names[i] = ''.join(syncnet_layer_names[i])\n",
    "\n",
    "        # Finding audio_start_idx\n",
    "        if not audio_found and 'audio' in syncnet_layer_names[i]:\n",
    "            audio_found = True\n",
    "            if verbose:\n",
    "                print(\"Found audio\")\n",
    "        elif not audio_found and 'audio' not in syncnet_layer_names[i]:\n",
    "            if 'conv' in syncnet_layer_names[i]:\n",
    "                audio_start_idx += 2\n",
    "            elif 'bn' in syncnet_layer_names[i]:\n",
    "                audio_start_idx += 3\n",
    "            elif 'fc' in syncnet_layer_names[i]:\n",
    "                audio_start_idx += 2\n",
    "\n",
    "        # Finding lip_start_idx\n",
    "        if not lip_found and 'lip' in syncnet_layer_names[i]:\n",
    "            lip_found = True\n",
    "            if verbose:\n",
    "                print(\"Found lip\")\n",
    "        elif not lip_found and 'lip' not in syncnet_layer_names[i]:\n",
    "            if 'conv' in syncnet_layer_names[i]:\n",
    "                lip_start_idx += 2\n",
    "            elif 'bn' in syncnet_layer_names[i]:\n",
    "                lip_start_idx += 3\n",
    "            elif 'fc' in syncnet_layer_names[i]:\n",
    "                lip_start_idx += 2\n",
    "\n",
    "        if verbose:\n",
    "            print(\"  \", i, syncnet_layer_names[i])\n",
    "\n",
    "    if verbose:\n",
    "        print(\"  lip_start_idx =\", lip_start_idx)\n",
    "        print(\"  audio_start_idx =\", audio_start_idx)\n",
    "\n",
    "    return syncnet_weights, syncnet_layer_names, audio_start_idx, lip_start_idx\n",
    "\n",
    "def set_syncnet_weights_to_syncnet_model(syncnet_model, syncnet_weights, syncnet_layer_names, mode, verbose):\n",
    "    ''' loading pre trained weights into the syncnet model layers '''\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Setting weights to model\")\n",
    "\n",
    "    # Video syncnet-related weights begin at 35 in syncnet_weights\n",
    "    if mode == 'lip':\n",
    "        syncnet_weights_idx = 35\n",
    "    else:\n",
    "        syncnet_weights_idx = 0\n",
    "\n",
    "    if mode == 'both':\n",
    "        syncnet_lip_model = syncnet_model[0]\n",
    "        syncnet_audio_model = syncnet_model[1]\n",
    "\n",
    "    # Init syncnet_layer_idx, to be incremented only at 'lip' layers\n",
    "    syncnet_layer_idx = -1\n",
    "\n",
    "    # Load weights layer-by-layer\n",
    "    for i in syncnet_layer_names:\n",
    "\n",
    "        # Skip the irrelevant layers\n",
    "        if mode == 'lip' and 'lip' not in i:\n",
    "            continue\n",
    "        elif mode == 'audio' and 'audio' not in i:\n",
    "            continue\n",
    "\n",
    "        # Increment the index on the model\n",
    "        syncnet_layer_idx += 1\n",
    "\n",
    "        if verbose:\n",
    "            print(\"  SyncNet Layer\", syncnet_layer_idx, \":\", i, \"; weight index :\", syncnet_weights_idx)\n",
    "\n",
    "        # Convolutional layer\n",
    "        if 'conv' in i:\n",
    "            syncnet_model.layers[syncnet_layer_idx].set_weights(\n",
    "                [np.transpose(syncnet_weights[syncnet_weights_idx], (2, 3, 1, 0)),\n",
    "                 np.squeeze(syncnet_weights[syncnet_weights_idx + 1])])\n",
    "            syncnet_weights_idx += 2\n",
    "\n",
    "        # Batch Normalization layer\n",
    "        elif 'bn' in i:\n",
    "            syncnet_model.layers[syncnet_layer_idx].set_weights(\n",
    "                [np.squeeze(syncnet_weights[syncnet_weights_idx]),\n",
    "                 np.squeeze(syncnet_weights[syncnet_weights_idx + 1]),\n",
    "                 syncnet_weights[syncnet_weights_idx + 2][0],\n",
    "                 syncnet_weights[syncnet_weights_idx + 2][1]])\n",
    "            syncnet_weights_idx += 3\n",
    "\n",
    "        # ReLU layer\n",
    "        elif 'relu' in i:\n",
    "            continue\n",
    "\n",
    "        # Pooling layer\n",
    "        elif 'pool' in i:\n",
    "            continue\n",
    "\n",
    "        # Dense (fc) layer\n",
    "        elif 'fc' in i:\n",
    "            # Skip Flatten layer\n",
    "            if 'flatten' in syncnet_model.layers[syncnet_layer_idx].name:\n",
    "                syncnet_layer_idx += 1\n",
    "            # Set weight to Dense layer\n",
    "            syncnet_model.layers[syncnet_layer_idx].set_weights(\n",
    "                [np.reshape(\n",
    "                    np.transpose(syncnet_weights[syncnet_weights_idx],\n",
    "                        (2, 3, 1, 0)),\n",
    "                    (syncnet_weights[syncnet_weights_idx].shape[2]*\\\n",
    "                     syncnet_weights[syncnet_weights_idx].shape[3]*\\\n",
    "                     syncnet_weights[syncnet_weights_idx].shape[1],\n",
    "                     syncnet_weights[syncnet_weights_idx].shape[0])),\n",
    "                np.squeeze(syncnet_weights[syncnet_weights_idx + 1])])\n",
    "            syncnet_weights_idx += 2\n",
    "\n",
    "def load_pretrained_syncnet_model(mode, verbose):\n",
    "    ''' final function to call loading functions here and prepare the final model'''\n",
    "\n",
    "    # mode = {lip, audio, both}\n",
    "    if mode not in {'lip', 'audio', 'both'}:\n",
    "        print(\"\\n\\nERROR: 'mode' not defined properly! Expected one of {'lip', 'audio', 'both'}, got:\", mode, \"\\n\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "\n",
    "        # Load syncnet model\n",
    "        syncnet_model = load_syncnet_model(mode=mode, verbose=verbose)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Loaded syncnet model\")\n",
    "\n",
    "        # Read weights and layer names\n",
    "        syncnet_weights, syncnet_layer_names, audio_start_idx, lip_start_idx = load_syncnet_weights(verbose=verbose)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Loaded syncnet weights.\")\n",
    "\n",
    "        # Set lip weights to syncnet_model\n",
    "        if mode != 'both':\n",
    "            set_syncnet_weights_to_syncnet_model(syncnet_model=syncnet_model,\n",
    "                                                 syncnet_weights=syncnet_weights,\n",
    "                                                 syncnet_layer_names=syncnet_layer_names,\n",
    "                                                 mode=mode,\n",
    "                                                 verbose=verbose)\n",
    "        else:\n",
    "            # Audio\n",
    "            set_syncnet_weights_to_syncnet_model(syncnet_model=syncnet_model[0],\n",
    "                                                 syncnet_weights=syncnet_weights,\n",
    "                                                 syncnet_layer_names=syncnet_layer_names,\n",
    "                                                 mode='audio',\n",
    "                                                 verbose=verbose)\n",
    "            # Lip\n",
    "            set_syncnet_weights_to_syncnet_model(syncnet_model=syncnet_model[1],\n",
    "                                                 syncnet_weights=syncnet_weights,\n",
    "                                                 syncnet_layer_names=syncnet_layer_names,\n",
    "                                                 mode='lip',\n",
    "                                                 verbose=verbose)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Set syncnet weights.\")\n",
    "\n",
    "    except ValueError as err:\n",
    "        print(err)\n",
    "        return\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\nCtrl+C was pressed!\\n\")\n",
    "        return\n",
    "\n",
    "    return syncnet_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tZziOxJiSZz-",
    "outputId": "c47401f2-4e29-491c-cc30-0fc3015b1024"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.engine.sequential.Sequential at 0x7f3d0fac7550>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x7f3cdf82fa58>]"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calling function to load model with weights\n",
    "\n",
    "mode = 'both'\n",
    "model=load_pretrained_syncnet_model( mode=mode, verbose=False)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "VaLUPemzSZxh"
   },
   "outputs": [],
   "source": [
    "# distance functions in tf\n",
    "\n",
    "def euclidean_distance_loss(y_true, y_pred):\n",
    "    ''' using tensorflow implementation to calculate distance '''\n",
    "\n",
    "    dists = tf.linalg.norm(y_pred - y_true, axis=1)\n",
    "    return dists\n",
    "\n",
    "def distance_euc_tf(feat1, feat2, vshift=15):\n",
    "  ''' takes 2 tensors as input and return euclidian distance between those '''\n",
    "  \n",
    "  win_size = vshift*2+1\n",
    "  paddings = tf.constant([[vshift, vshift+1,], [0, 0]])\n",
    "  feat2p = tf.pad(feat2, paddings, \"CONSTANT\")              # padding for feat2\n",
    "  \n",
    "  if len(feat2p) < len(feat1)+win_size:\n",
    "    # after padding in feat2, if still not getting enough rows to calculate distance in below for loop, we have to pad 'n' more rows\n",
    "\n",
    "    n = len(feat1)+win_size-len(feat2p)\n",
    "    padd = tf.constant([[0, n,], [0, 0]])\n",
    "    feat2p = tf.pad(feat2p, padd, \"CONSTANT\")\n",
    "\n",
    "  dists = []\n",
    "\n",
    "  # we have to create pairwise distance, so running below for loop so it'll calculate distance of every row of feat1 with every 31 rows sample of feat2p\n",
    "\n",
    "  for i in range(0,len(feat1)):\n",
    "    a=tf.repeat([feat1[i,:]], win_size, axis=0)\n",
    "    b=feat2p[i:i+win_size,:]\n",
    "    dists.append(euclidean_distance_loss(a, b))\n",
    "\n",
    "  mdist = np.mean(np.stack(dists,1),1)     # mdist will be array of 31 values\n",
    "  mdist = tf.convert_to_tensor(mdist)\n",
    "  return mdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "IPWPAJFfSZqL"
   },
   "outputs": [],
   "source": [
    "def loss_function(y_true, pred_dist):\n",
    "  ''' calculates contrastive loss between true and predictive values '''\n",
    "\n",
    "  e=0\n",
    "  for i in range(31):\n",
    "    e = e + (y_true[i]*(pred_dist[i])**2) + ((1-y_true[i])*max(1-pred_dist[i],0)**2)\n",
    "  loss = e/(2*31)\n",
    "  \n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "uZkiLUDoS1QU"
   },
   "outputs": [],
   "source": [
    "def final_fun_1(video):\n",
    "\n",
    "    MOUTH_H = 112\n",
    "    MOUTH_W = 112\n",
    "    FACE_H = 224\n",
    "    FACE_W = 224\n",
    "    MOUTH_TO_FACE_RATIO = 0.65\n",
    "    SYNCNET_VIDEO_FPS = 25\n",
    "    SYNCNET_VIDEO_CHANNELS = int(0.2 * SYNCNET_VIDEO_FPS)  # 5\n",
    "    SYNCNET_MFCC_CHANNELS = 12\n",
    "    AUDIO_TIME_STEPS = 20\n",
    "    IMAGE_DATA_FORMAT = 'channels_last'\n",
    "    \n",
    "    user=video.split('_')[2]\n",
    "    user=user.split('/')[1]\n",
    "    au=video.split('_')[-1]\n",
    "    au=au.split('.')[0]\n",
    "\n",
    "    video_fea=video_processing(video)\n",
    "    audio_fea=audio_processing('/content/gdrive/My Drive/VIDTIMIT/'+user+'/audio/'+au+'.wav',False)\n",
    "    #print('video and audio features respectively :',video_fea.shape,audio_fea.shape)\n",
    "\n",
    "    audio_pred = model[0].predict(audio_fea)\n",
    "    lip_pred = model[1].predict(video_fea)\n",
    "    \n",
    "    return lip_pred, audio_pred\n",
    "\n",
    "def final_fun_2(lip_pred, audio_pred, y_true):\n",
    "    d = distance_euc_tf(lip_pred, audio_pred)\n",
    "    #print('distance :',d)\n",
    "\n",
    "    conf = np.median(d)-min(d)\n",
    "    if conf>3.5:\n",
    "      print ('video is real')\n",
    "    else:\n",
    "      print ('video is fake')\n",
    "\n",
    "    l=loss_function(y_true,d)\n",
    "    return l\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FDCwiwoDUl-c",
    "outputId": "541bce80-2fd6-4a02-e935-5071e1855c0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video predicted shape (23, 128)\n",
      "audio predicted shape (23, 128)\n",
      "video is fake\n",
      "loss is : tf.Tensor(72.666695, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# calling final function\n",
    "\n",
    "video='/content/gdrive/My Drive/vidtimit_videos/tampered/fadg0_sa1.mp4'\n",
    "\n",
    "lip_pred, audio_pred = final_fun_1(video)       # calling function 1\n",
    "print('video predicted shape',lip_pred.shape)\n",
    "print('audio predicted shape',audio_pred.shape)\n",
    "\n",
    "y_true=[0 for i in range(31)]\n",
    "loss = final_fun_2(lip_pred, audio_pred, y_true)  # calling function 2\n",
    "print('loss is :', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yJTKEJSgWgpt"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
